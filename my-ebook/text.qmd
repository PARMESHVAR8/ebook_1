---
title: "Basic Statistics using GUI-R (RKWard) - Week 5"
author: "Harsh Pradhan, Assistant Professor, Institute of Management Studies, BHU"
date: "`r Sys.Date()`"
format: 
  html:
    toc: true
    toc-depth: 3
    code-fold: false
    theme: cosmo
    css: styles.css
editor: visual
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
library(tidyverse)
library(knitr)
library(ggplot2)
library(HH)
library(ggm)
```

# Module 1: One-Way ANOVA

## Introduction to ANOVA

Analysis of Variance (ANOVA) is a statistical technique used to compare means across multiple groups. It helps determine whether there are statistically significant differences between the means of three or more independent groups.

## Key Concepts

### Sum of Squares (SS)

The ANOVA procedure involves calculating different types of sum of squares:

1. **Total Sum of Squares (SST)**: Total variation in the data
2. **Between-Groups Sum of Squares (SSB)**: Variation between group means
3. **Within-Groups Sum of Squares (SSW)**: Variation within each group

### Model (Between-Group) Sum of Squares Calculation

To calculate the Model Sum of Squares (SS_M):

1. Calculate the difference between the mean of each group and the grand mean
2. Square each of these differences
3. Multiply each result by the number of participants within that group
4. Add the values for each group together

## ANOVA Table Structure

```{r anova-table-example}
# Creating an example ANOVA table
anova_table <- data.frame(
  Source = c("Between", "Within", "Total"),
  SS = c(461.64, 167.42, 629.08),
  df = c(3, 9, 12),
  MS = c(153.88, 18.60, NA),
  F = c(8.27, NA, NA),
  p = c("<.001", NA, NA)
)

kable(anova_table, caption = "One-Way ANOVA Table Example")
```

## Degrees of Freedom

- **Between groups df**: k - 1 (where k = number of groups)
- **Within groups df**: N - k (where N = total sample size)
- **Total df**: N - 1

## Mean Squares Calculation

Mean Square is calculated by dividing Sum of Squares by corresponding degrees of freedom:

- MS_M = SS_M / df_M
- MS_R = SS_R / df_R

## F-Ratio

The F test statistic is the ratio of Mean Square Between to Mean Square Within:

**F = MS_M / MS_R**

A larger F ratio indicates a larger difference between group means relative to variation within groups.

## Example: Fairness in Different Types of Societies

```{r fairness-example}
# Data from the fairness study
fairness_data <- data.frame(
  Hunter_gatherer = c(28, 36, 38, 31),
  Farming = c(32, 33, 40, NA),
  Natural_resources = c(47, 43, 52, NA),
  Industrial = c(40, 47, 45, NA)
)

# Calculate means
means <- c(33.25, 35.0, 47.33, 44.0)
grand_mean <- 39.385

# Display the data
kable(fairness_data, caption = "Fairness Scores by Society Type")
```

## Hypothesis Testing

The F test statistic follows an F distribution with:
- Numerator df = df_M
- Denominator df = df_R

For our example: F(3, 9) = 8.27, p < .001

```{r f-test-example}
# Calculate F critical value
f_critical <- qf(0.95, 3, 9)
print(paste("F critical value (α = 0.05):", round(f_critical, 3)))

# Our calculated F value
f_calculated <- 8.27
print(paste("Calculated F value:", f_calculated))
print(paste("Significant?", f_calculated > f_critical))
```

# Module 2: Types of ANOVA

## Repeated Measures vs Between-Subjects ANOVA

```{r anova-comparison}
comparison_table <- data.frame(
  Aspect = c("Experimental Design", "Dependency", "Statistical Power", 
             "Control of Individual Differences", "Efficiency", "Example Research Question"),
  Repeated_Measures = c(
    "Subjects tested under multiple conditions",
    "Assumes dependency between measures on same subject",
    "May have lower power due to within-subject variability",
    "Each subject serves as their own control",
    "Often requires fewer subjects for equivalent power",
    "Does a new teaching method result in improved test scores?"
  ),
  Between_Subjects = c(
    "Different groups exposed to different conditions",
    "Assumes independence between subjects in different groups",
    "Generally has higher power when number of subjects is large",
    "Individual differences between subjects can introduce noise",
    "Can be less efficient in terms of sample size requirements",
    "Do different teaching methods result in different test scores?"
  )
)

kable(comparison_table, caption = "Repeated Measures vs Between-Subjects ANOVA")
```

## Between-Subjects ANOVA Example

```{r between-subjects-example}
# Example ANOVA table for between-subjects design
between_subjects_table <- data.frame(
  Source = c("Between-Subjects", "Within-Subjects", "Total"),
  SS = c(86.667, 1600, 1686.667),
  df = c(2, 87, 89),
  MS = c(43.3335, 18.3908, NA),
  F = c(2.3552, NA, NA)
)

kable(between_subjects_table, caption = "Between-Subjects ANOVA Example")

# F critical value
f_critical_bs <- qf(0.95, 2, 87)
print(paste("F critical value:", round(f_critical_bs, 3)))
```

## MANOVA vs N-way ANOVA

```{r manova-nway-comparison}
manova_nway_table <- data.frame(
  Aspect = c("Use", "Objective", "Assumption", "Example"),
  MANOVA = c(
    "Multiple dependent variables, one or more independent variables",
    "Determine differences between groups in multivariate response",
    "Multivariate normality, equal covariance matrices",
    "Teaching methods' effect on exam scores, participation, homework"
  ),
  N_way_ANOVA = c(
    "Multiple independent variables affecting single dependent variable",
    "Examine interaction effects between multiple independent variables",
    "Normal distribution, homogeneous variances",
    "Temperature and humidity effects on plant growth"
  )
)

kable(manova_nway_table, caption = "MANOVA vs N-way ANOVA Comparison")
```

## Two-Way ANOVA Example

```{r two-way-anova-example, eval=FALSE}
# Example code for two-way ANOVA
# Assuming data file 'nanova.csv' exists
# data <- read.csv("nanova.csv")

# Fit the ANOVA model
# model <- aov(Yield ~ Treatment * Dose, data = data)

# View the ANOVA summary
# summary(model)

# Interaction plot
# HH::interaction2wt(data$Yield ~ data$Treatment * data$Dose)
```

## ANCOVA (Analysis of Covariance)

ANCOVA combines ANOVA with regression analysis. It examines whether population means of a dependent variable are equal across levels of a categorical independent variable, while statistically controlling for the effects of other continuous variables (covariates).

# Module 3: Correlation Analysis

## Types of Correlation

```{r correlation-types}
correlation_types <- data.frame(
  Data_Type = c("Nominal", "Dichotomous", "Ordinal/Rank", "Ratio/Interval", "Partial"),
  Type_of_Correlation = c("Phi", "Bi-serial", "Spearman/Kendall", "Pearson", "Partial Correlation")
)

kable(correlation_types, caption = "Types of Correlation by Data Type")
```

## R Commands for Correlation

```{r correlation-commands, eval=FALSE}
# Pearson correlation test
stats::cor.test(my.csv.data$JP_01, my.csv.data$JP_02, 
                alternative="two.sided", conf.level=0.95)

# Partial correlation
ggm::pcor(my.csv.data$JP_01, my.csv.data$JP_02, my.csv.data$JP_03)

# Through RKWard GUI:
# Statistics → Summaries → Correlation matrix
# Teaching → Regression → Correlation
```

## Comparison of Correlation Methods

```{r correlation-methods-comparison}
correlation_comparison <- data.frame(
  Method = c("Pearson", "Spearman", "Kendall"),
  Description = c(
    "Measures linear relationship between two continuous variables",
    "Measures monotonic relationship between two variables",
    "Measures monotonic relationship between two variables"
  ),
  Assumptions = c(
    "Normal distribution, linearity",
    "No linearity, suitable for ordinal or ranked data",
    "No linearity, suitable for ordinal or ranked data"
  ),
  Suitable_for = c(
    "Continuous variables",
    "Ordinal or ranked data",
    "Ordinal or ranked data"
  ),
  Sensitivity_to_Outliers = c("Sensitive", "Less sensitive", "Robust for small size"),
  R_Command = c(
    'cor(x, y, method = "pearson")',
    'cor(x, y, method = "spearman")',
    'cor(x, y, method = "kendall")'
  )
)

kable(correlation_comparison, caption = "Comparison of Correlation Methods")
```

## Uses of Correlation

1. **Grouping Similar Variables**: Identifying variables that measure similar constructs
2. **Reliability Analysis**: Assessing internal consistency of measurement instruments

# Module 4: Introduction to Regression

## Linear vs Non-linear Regression

### Linear Regression
Linear regression analysis is used to predict the value of a variable based on the value of another variable. The variable you want to predict is called the dependent variable. The variable you are using to predict the other variable's value is called the independent variable.

### Non-linear Regression
Nonlinear regression is a statistical technique that helps describe nonlinear relationships in experimental data. Nonlinear regression models are generally assumed to be parametric, where the model is described as a nonlinear equation. Typically machine learning methods are used for non-parametric nonlinear regression.

## Regression Analysis Example

```{r regression-example}
# Example dataset creation
set.seed(123)
X <- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)
Y <- c(2.1, 3.9, 6.2, 7.8, 10.1, 12.3, 13.8, 16.2, 18.1, 20.0)

# Create dataframe
regression_data <- data.frame(X = X, Y = Y)

# Display the data
kable(regression_data, caption = "Example Regression Dataset")

# Plot the data
ggplot(regression_data, aes(x = X, y = Y)) +
  geom_point(color = "blue", size = 3) +
  geom_smooth(method = "lm", se = FALSE, color = "red") +
  theme_minimal() +
  labs(title = "Linear Regression Example",
       x = "Independent Variable (X)",
       y = "Dependent Variable (Y)")
```

## Regression Formulas

The key formulas for simple linear regression are:

### Slope (b)
**b = SSxy / SSxx**

### Intercept (a)
**a = ȳ - b * x̄**

### Coefficient of Determination (R²)
**R² = 1 - (SSE / SST)**

Where:
- SST = Total Sum of Squares = Σ(y - ȳ)²
- SSE = Sum of Squared Errors
- SSxy = Sum of cross products
- SSxx = Sum of squares of X

### Degrees of Freedom
- DF_regression = 1 (for simple linear regression)
- DF_total = n - 1

```{r regression-calculations}
# Calculate regression statistics
x_bar <- mean(X)
y_bar <- mean(Y)

# Calculate sums of squares
SSxx <- sum((X - x_bar)^2)
SSyy <- sum((Y - y_bar)^2)
SSxy <- sum((X - x_bar) * (Y - y_bar))

# Calculate slope and intercept
b <- SSxy / SSxx
a <- y_bar - b * x_bar

# Fit the model
model <- lm(Y ~ X, data = regression_data)

# Display results
cat("Regression Statistics:\n")
cat("Slope (b):", round(b, 3), "\n")
cat("Intercept (a):", round(a, 3), "\n")
cat("R-squared:", round(summary(model)$r.squared, 3), "\n")

# ANOVA table for regression
anova_reg <- anova(model)
kable(anova_reg, caption = "ANOVA Table for Regression")
```

## Practical Exercise

To practice these concepts:

1. Create a dataset with variables X and Y
2. Perform correlation analysis
3. Conduct linear regression
4. Interpret the results

```{r practical-exercise, eval=FALSE}
# Step 1: Create dataset
# (Use your own data or generate sample data)

# Step 2: Correlation analysis
# cor.test(X, Y)

# Step 3: Linear regression
# model <- lm(Y ~ X)
# summary(model)

# Step 4: Visualization
# plot(X, Y)
# abline(model, col = "red")
```

---

## Summary

This document covers the essential concepts of:

1. **One-Way ANOVA**: Understanding variance components and F-tests
2. **Types of ANOVA**: Repeated measures, between-subjects, MANOVA, and ANCOVA
3. **Correlation**: Different types and their applications
4. **Regression**: Linear relationships and prediction

These statistical techniques form the foundation for more advanced analyses in research and data science applications.