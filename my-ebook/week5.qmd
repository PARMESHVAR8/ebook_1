title: "Week5"
author: "Dr. Harsh Pradhan"
affiliation: "Institute of Management Studies, Banaras Hindu University"
format: 
  html:
    toc: true
    toc-depth: 4
    number-sections: true
    code-fold: true
    theme: cosmo
    css: styles.css
  pdf:
    toc: true
    number-sections: true
    geometry: margin=1in
    fontsize: 11pt
    documentclass: article
bibliography: references.bib
---

# 1. Introduction

Welcome to **Week 5** of _Basic Statistics using GUI-R (RKWard)_, where we cover **relationship testing**, **correlation**, **regression**, **ANOVA**, and related diagnostics in **depth**, using both theory and R-based implementation.

This book is designed to:

- Clarify statistical concepts visually
- Use real data simulations
- Empower you with reproducible R/RKWard workflows
- Prepare you to run statistical diagnostics and build interpretations

---

# 2. Lecture 24 â€“ Deep Dive: Correlation

## 2.1 What is Correlation?

Correlation is a statistical measure that expresses the **extent to which two variables are linearly related**.

### ðŸ’¡ Theory

- If variable X increases as Y increases â†’ **Positive correlation**
- If variable X increases as Y decreases â†’ **Negative correlation**
- If there's no linear trend â†’ **Zero correlation**

> Pearsonâ€™s r ranges from -1 to +1.

---

## 2.2 Types of Correlation and Use Cases

| Data Type          | Correlation Type | Use Case Example                      |
|--------------------|------------------|----------------------------------------|
| Nominal            | Phi              | Gender vs. Yes/No Preferences         |
| Dichotomous        | Point-Biserial   | Pass/Fail vs. Exam Score              |
| Ordinal/Rank       | Spearman/Kendall | Rank in class vs. Test anxiety        |
| Ratio/Interval     | Pearson          | Height vs. Weight                     |
| Multivariate       | Partial Correl.  | Control confounders                   |

---

## 2.3 Pearson, Spearman, Kendall Comparison

{r}
# Simulate linear data
set.seed(123)
x <- rnorm(100)
y <- 2 * x + rnorm(100)

# Add non-linear data
z <- x^2 + rnorm(100)

# Pearson (linear)
cor(x, y, method = "pearson")

# Spearman (rank, monotonic)
cor(x, z, method = "spearman")

# Kendall (ordinal)
cor(x, z, method = "kendall")
2.4 Visualizing Correlations
# Visualization
library(ggplot2)
data <- data.frame(x, y, z)

ggplot(data, aes(x = x, y = y)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "blue") +
  labs(title = "Scatter Plot with Linear Fit", x = "X", y = "Y")

ggplot(data, aes(x = x, y = z)) +
  geom_point(color = "darkred") +
  labs(title = "Non-Linear Relationship", x = "X", y = "Z")
2.5 Correlation Matrix in RKWard
Steps:

Load data into RKWard.

Navigate to Statistics â†’ Summaries â†’ Correlation Matrix.

Choose the appropriate variables.

Choose correlation type (Pearson, Spearman).

Run and interpret the matrix output.

2.6 Partial Correlation in R
When you want to compute the correlation between two variables while controlling for a third:

# install.packages("ggm")
library(ggm)
X1 <- rnorm(100)
X2 <- X1 + rnorm(100, sd = 0.5)
X3 <- rnorm(100)
pcor(c("X1", "X2", "X3"), cov(cbind(X1, X2, X3)))
Interpretation: This tells you the pure correlation between X1 and X2, controlling for X3.

2.7 R Code to Automate All

# Simulate data
set.seed(100)
data <- data.frame(
  A = rnorm(100),
  B = rnorm(100),
  C = rnorm(100)
)

# Generate all pairwise correlations
cor(data)

# Visualize matrix with corrplot
library(corrplot)
corrplot(cor(data), method = "color", tl.col = "black", addCoef.col = "black")
2.8 Spearman vs Pearson â€“ When to Use?
Use Pearson when data is normally distributed, continuous, and linear.

Use Spearman when data is ordinal, ranked, or non-linear but monotonic.

Kendallâ€™s Tau is more robust for small sample sizes.

ðŸ‘€ **Next Up: Part 2/4** will include:

- One-Way ANOVA full theory + math  
- Repeated Measures ANOVA (detailed)  
- Visualization of F-distributions  
- MANOVA + N-Way examples  
- 10+ R code exercises


# 3. Lecture 25 â€“ One-Way ANOVA (Detailed)

## 3.1 Concept Overview

**Analysis of Variance (ANOVA)** is used when comparing the **means of three or more groups**.

### ðŸ’¡ Formula Breakdown

- **SSM (Sum of Squares Model)**: Variation between groups
- **SSR (Sum of Squares Residual)**: Variation within groups
- **SST (Total)**: Total variation

**F-Ratio**:

$$
F = \frac{MS_{between}}{MS_{within}} = \frac{SSM / df_M}{SSR / df_R}
$$


## 3.2 ANOVA Table Example

| Source   | SS     | df  | MS     | F    |
|----------|--------|-----|--------|------|
| Between  | 461.64 | 3   | 153.88 | 8.27 |
| Within   | 167.42 | 9   | 18.60  |      |
| Total    | 629.08 | 12  |        |      |

---

## 3.3 R Code â€“ One-Way ANOVA

```{r}
group1 <- c(28, 36, 38, 31)
group2 <- c(32, 33, 40)
group3 <- c(47, 43, 52)
group4 <- c(40, 47, 45)

score <- c(group1, group2, group3, group4)
group <- factor(rep(c("Hunter", "Farming", "Natural", "Industrial"), times=c(4,3,3,3)))

data <- data.frame(score, group)
anova_model <- aov(score ~ group, data=data)
summary(anova_model)
3.4 Post-Hoc Analysis (Tukey HSD)

TukeyHSD(anova_model)
3.5 Visualize Group Differences

boxplot(score ~ group, data = data, col = c("lightblue", "pink", "lightgreen", "yellow"))
4. Lecture 26 â€“ Repeated Measures ANOVA
4.1 Theory
Repeated measures involve the same subjects measured under multiple conditions.

Aspect	Repeated Measures	Between-Subjects
Subjects	Same across treatments	Different per group
Variability Control	Higher (less noise)	Lower
Efficiency	More efficient	Requires more samples

4.2 R Code â€“ Repeated Measures

library(ez)
subject <- factor(rep(1:10, each=3))
treatment <- factor(rep(c("Pre", "Mid", "Post"), times=10))
score <- c(rnorm(10, 65), rnorm(10, 70), rnorm(10, 75))
rm_df <- data.frame(subject, treatment, score)

ezANOVA(data=rm_df, dv=score, wid=subject, within=treatment)
4.3 Visual Check

library(ggplot2)
ggplot(rm_df, aes(x=treatment, y=score, group=subject, color=subject)) +
  geom_line() + geom_point() +
  theme_minimal() +
  labs(title="Repeated Measures ANOVA Plot")
5. Lecture 27 â€“ MANOVA and N-Way ANOVA
5.1 What is MANOVA?
Multivariate Analysis of Variance (MANOVA) extends ANOVA to multiple dependent variables.

Example Use Case:

Investigating how teaching methods affect:

Exam scores

Class participation

Homework submission

5.2 R Code â€“ MANOVA

y1 <- rnorm(30, 60, 5)
y2 <- rnorm(30, 70, 6)
y3 <- rnorm(30, 80, 4)
method <- factor(rep(c("A", "B", "C"), each=10))

manova_model <- manova(cbind(y1, y2, y3) ~ method)
summary(manova_model)
5.3 N-Way ANOVA (Interaction Effects)

df <- expand.grid(
  Teaching = c("Traditional", "Interactive"),
  Gender = c("Male", "Female"),
  Rep = 1:20
)
df$Score <- rnorm(80, mean = 70, sd = 5)

model_nway <- aov(Score ~ Teaching * Gender, data = df)
summary(model_nway)
5.4 Interaction Plot
{r}
interaction.plot(df$Teaching, df$Gender, df$Score, col=c("red", "blue"))
5.5 Assumptions of ANOVA
Assumption	Check Method	Tool
Normality	QQ Plot, Shapiro Test	shapiro.test()
Homogeneity	Leveneâ€™s/Bartlettâ€™s Test	car::leveneTest()
Independence	Design-level assurance	Design phase

5.6 Assumption Check in R
{r}
# Normality check
shapiro.test(residuals(anova_model))

# Homogeneity check
library(car)
leveneTest(score ~ group, data = data)
5.7 Visualizing F-Distribution

curve(df(x, df1=3, df2=9), from=0, to=10, col="blue", lwd=2,
      ylab="Density", main="F-distribution df(3,9)")
abline(v=8.27, col="red", lwd=2, lty=2)
legend("topright", legend=c("F = 8.27"), col="red", lty=2)
5.8 Simulation: When F is not significant

set.seed(2024)
group_A <- rnorm(10, mean=50)
group_B <- rnorm(10, mean=51)
group_C <- rnorm(10, mean=50.5)

score <- c(group_A, group_B, group_C)
group <- factor(rep(c("A", "B", "C"), each=10))

df <- data.frame(score, group)
aov_model <- aov(score ~ group, data=df)
summary(aov_model)
âž¡ï¸ End of Part 2/4. Part 3 includes Regression (Simple, Multiple, Non-linear), VIF, Residuals, and Advanced Modeling


# 6. Lecture 28 â€“ Simple Linear Regression

## 6.1 Theory Refresher

Linear regression predicts a **dependent variable (Y)** using an **independent variable (X)**.

**Model Equation**:

$$
Y = \beta_0 + \beta_1 X + \epsilon
$$

Where:

- $\beta_0$ = Intercept  
- $\beta_1$ = Slope  
- $\epsilon$ = Error term

---

## 6.2 Example in R

```{r}
study_time <- c(2, 3, 4, 5, 6)
grades <- c(50, 60, 65, 70, 75)

model <- lm(grades ~ study_time)
summary(model)
6.3 Regression Line Visualization

plot(study_time, grades, main="Simple Regression", xlab="Study Time", ylab="Grades")
abline(model, col="blue", lwd=2)
6.4 Interpret Coefficients

coef(model)
Intercept: Grade when study time = 0

Slope: Grade increases per hour of study

6.5 Residual Plots

par(mfrow=c(2,2))
plot(model)
Top-left: Residuals vs Fitted

Bottom-left: Scale-Location

Top-right: QQ Plot

Bottom-right: Residuals vs Leverage

6.6 Confidence Intervals

confint(model)
7. Lecture 29 â€“ Multiple Regression
7.1 Add More Predictors

df <- data.frame(
  Exam = c(50, 55, 60, 65, 70),
  Hours = c(2, 3, 4, 5, 6),
  Sleep = c(7, 6.5, 6, 5.5, 5)
)
multi_model <- lm(Exam ~ Hours + Sleep, data = df)
summary(multi_model)
7.2 Check VIF (Multicollinearity)

library(car)
vif(multi_model)
VIF > 5 â†’ multicollinearity warning
VIF > 10 â†’ serious problem

7.3 Partial Residual Plots

avPlots(multi_model)
7.4 Plot 3D Regression Plane

## install.packages("scatterplot3d")
library(scatterplot3d)
scatterplot3d(df$Hours, df$Sleep, df$Exam, highlight.3d=TRUE, type="h",
              angle=55, color="darkgreen", pch=16)
8. Lecture 30 â€“ Polynomial and Non-Linear Regression
8.1 Simulating Non-linear Relationship

x <- seq(0, 10, 0.1)
y <- 5 + 2 * x^2 + rnorm(length(x), 0, 5)
plot(x, y, main="Non-linear Pattern", pch=19)
8.2 Polynomial Regression

poly_model <- lm(y ~ poly(x, 2))
summary(poly_model)

lines(x, predict(poly_model), col="blue", lwd=2)
8.3 Compare with Linear Fit

linear_model <- lm(y ~ x)
lines(x, predict(linear_model), col="red", lwd=2, lty=2)
legend("topleft", legend=c("Poly", "Linear"), col=c("blue", "red"), lty=c(1,2))
8.4 Residual Analysis

par(mfrow=c(1,2))
plot(poly_model$fitted.values, poly_model$residuals, main="Polynomial Residuals")
plot(linear_model$fitted.values, linear_model$residuals, main="Linear Residuals")
8.5 Curve Fitting with nls()

x <- seq(0, 10, length.out=100)
y <- 2 * exp(0.3 * x) + rnorm(100, sd=3)

nls_model <- nls(y ~ a * exp(b * x), start=list(a=2, b=0.3))
summary(nls_model)

lines(x, predict(nls_model), col="purple", lwd=2)
9. Lecture 31 â€“ Model Evaluation Metrics
9.1 RÂ² and Adjusted RÂ²

summary(multi_model)$r.squared
summary(multi_model)$adj.r.squared
9.2 MSE and RMSE

pred <- predict(multi_model)
actual <- df$Exam
residuals <- actual - pred
mse <- mean(residuals^2)
rmse <- sqrt(mse)

# 10. Lecture 32 â€“ Logistic Regression

## 10.1 When to Use

Logistic regression is used when the **dependent variable is categorical** (typically binary: 0/1, Yes/No, Pass/Fail).

## 10.2 Logistic Function

$$
P(Y=1) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 X)}}
$$

---

## 10.3 R Example: Predicting Admission

```{r}
df <- data.frame(
  Admit = c(1,1,0,1,0,0,1,1,0,0),
  Score = c(80,85,60,90,55,40,88,83,59,52)
)

logit_model <- glm(Admit ~ Score, data=df, family="binomial")
summary(logit_model)
10.4 Probability Prediction

df$Prob <- predict(logit_model, type="response")
df
10.5 ROC Curve

library(pROC)
roc_obj <- roc(df$Admit, df$Prob)
plot(roc_obj, col="darkgreen")
auc(roc_obj)
10.6 Classification Table

df$Pred <- ifelse(df$Prob > 0.5, 1, 0)
table(Predicted = df$Pred, Actual = df$Admit)
11. Lecture 33 â€“ Chi-Square Test
11.1 Categorical Independence
Used when evaluating if two categorical variables are independent.

11.2 Example: Gender vs Department Choice

gender <- c("Male", "Male", "Female", "Female")
dept <- c("Science", "Arts", "Science", "Arts")
counts <- c(30, 20, 25, 25)

chi_df <- data.frame(Gender=rep(gender, counts), Dept=rep(dept, counts))
tbl <- table(chi_df$Gender, chi_df$Dept)
chisq.test(tbl)
12. Lecture 34 â€“ Non-Parametric Tests
12.1 When to Use
Data is not normally distributed

Ordinal data or small sample sizes

12.2 Mannâ€“Whitney U

group1 <- c(45, 50, 60, 55)
group2 <- c(70, 75, 80, 85)
wilcox.test(group1, group2)
12.3 Kruskalâ€“Wallis (Non-parametric ANOVA)

g1 <- c(10, 20, 30)
g2 <- c(40, 50, 60)
g3 <- c(70, 80, 90)
kw_df <- data.frame(
  score = c(g1, g2, g3),
  group = factor(rep(c("A", "B", "C"), each=3))
)
kruskal.test(score ~ group, data=kw_df)
12.4 Wilcoxon Signed-Rank

before <- c(60, 70, 65, 80)
after <- c(62, 75, 68, 82)
wilcox.test(before, after, paired=TRUE)
13. Case Study â€“ Social Media & Mental Health
13.1 Dataset Simulation

set.seed(100)
n <- 100
hours <- rnorm(n, 3, 1.5)
stress <- 10 + 1.2 * hours + rnorm(n)

df <- data.frame(hours, stress)
model <- lm(stress ~ hours, data=df)
summary(model)
13.2 Visual

plot(hours, stress, main="Social Media Use vs Stress", pch=19)
abline(model, col="red", lwd=2)
13.3 Interpretation
Positive slope â†’ More hours = more stress

RÂ² tells how well hours predict stress

14. 50 Multiple Choice Questions (MCQs)
Q1. Pearsonâ€™s r is best used when:
 Data is ordinal

 Data is continuous and normally distributed

 Data has outliers

 You want to rank variables

Q2. Which test compares more than 2 independent means?
 t-test

 ANOVA

 Chi-Square

 Correlation

Q3. A VIF of 12 means:
 No multicollinearity

 Severe multicollinearity

 Perfect fit

 Homoscedasticity


15. Exercises
Exercise 1: One-Way ANOVA on Fake Marketing Data
Generate three ad strategies and test which gives highest customer conversions.

Exercise 2: Correlate temperature and ice cream sales
Include scatterplot, Pearson's r, regression line.

Exercise 3: Logistic regression predicting credit approval
Predict using income and debt ratio.

Exercise 4: Chi-Square on survey data
Test independence of satisfaction vs. purchase intention.

Exercise 5: Repeated Measures ANOVA
Simulate 10 people tested across 3 time points.

16. Glossary
Term	Definition
ANOVA	Test for differences in means across groups
Regression	Predict numerical output from inputs
Correlation	Measure of linear association between two variables
RÂ²	Proportion of variance explained by model
AIC	Akaike Information Criterion â€“ model quality metric
VIF	Variance Inflation Factor â€“ checks multicollinearity
Logistic Regression	Used for binary outcome prediction
Chi-Square	Test for independence between two categorical variables

17. Appendix
17.1 RKWard Menus
Correlation â†’ Statistics â†’ Summaries â†’ Correlation Matrix

ANOVA â†’ Analysis â†’ ANOVA â†’ One-Way or Repeated

Plots â†’ Graphics â†’ Histogram / Boxplot / Scatterplot

Regression â†’ Analysis â†’ Linear Models

17.2 Troubleshooting
Issue	Solution
â€œobject not foundâ€	Check variable names (case-sensitive)
Plot doesn't show	Use print(plot_name) or run outside R chunk
Model output blank	Use summary(model) instead of just model
Package not found	Install using install.packages("name")

18. References
Field, A. (2013). Discovering Statistics Using R.

R Documentation â€“ https://www.rdocumentation.org

BHU Faculty Portal â€“ Dr. Harsh Pradhan

RKWard Guidebook â€“ https://rkward.kde.org